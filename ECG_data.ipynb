{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dbe3865-9c37-40b0-9016-e544ef91d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from scipy import stats\n",
    "from biosppy.signals import ecg\n",
    "import neurokit2 as nk\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732314ca-9d68-4bbb-8e17-4af22335de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "train_labels = train_data['y']\n",
    "train_data = train_data.drop(columns=['id', 'y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1dbf8f-bcce-47ca-9f1a-ea20b591cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hjorth_parameters(signal):\n",
    "    first_derivative = np.diff(signal)\n",
    "    second_derivative = np.diff(signal, n=2)\n",
    "    var_zero = np.var(signal)\n",
    "    var_d1 = np.var(first_derivative)\n",
    "    var_d2 = np.var(second_derivative)\n",
    "    if var_zero == 0 or var_d1 == 0 or var_d2 == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    activity = var_zero\n",
    "    mobility = np.sqrt(var_d1 / var_zero)\n",
    "    complexity = np.sqrt(var_d2 / var_d1)\n",
    "    return activity, mobility, complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3dee56-9814-4d5c-975a-ab18d68688a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "counter=0\n",
    "\n",
    "def extract_features(row, sampling_rate=300):\n",
    "    # drop NaN values\n",
    "    global counter\n",
    "    signal_raw = row.dropna().values\n",
    "    \n",
    "    if len(signal_raw) < sampling_rate * 2:  # Ensure at least 2 seconds of data\n",
    "        print(\"Signal too short, skipping feature extraction.\")\n",
    "        counter= counter +1\n",
    "        return features\n",
    "\n",
    "    # a dictionary to store features of the current signal\n",
    "    features = {}\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Noise Reduction\n",
    "        \n",
    "        # Bandpass Filter between 0.5 Hz and 40 Hz ? to recheck \n",
    "        nyquist = 0.5 * sampling_rate\n",
    "        low = 0.5 / nyquist\n",
    "        high = 40 / nyquist\n",
    "        b, a = signal.butter(5, [low, high], btype='band')\n",
    "        signal_filtered = signal.filtfilt(b, a, signal_raw)\n",
    "    \n",
    "        # Denoising using ECG Clean function\n",
    "        signal_denoised = nk.ecg_clean(signal_filtered, sampling_rate=sampling_rate, method='neurokit')\n",
    "        signal_denoised = signal_denoised[~np.isnan(signal_denoised)]\n",
    "        # *************** function nk.ecg_process does the cleaning too. should I remove this one, or is it still necessary????\n",
    "\n",
    "        r_peaks_m = ecg.engzee_segmenter(signal_denoised, 300)['rpeaks']\n",
    "        beats = ecg.extract_heartbeats(signal_denoised, r_peaks_m, 300)['templates']\n",
    "        mu = np.mean(beats, axis=0) \n",
    "        \n",
    "        # Basic Statistical Features\n",
    "        features['signal_mean'] = np.mean(signal_denoised)\n",
    "        features['signal_std'] = np.std(signal_denoised) \n",
    "        features['signal_min'] = np.min(signal_denoised)\n",
    "        features['signal_max'] = np.max(signal_denoised)\n",
    "        features['signal_skewness'] = stats.skew(signal_denoised) # Should be on the average heartbeat\n",
    "        features['signal_kurtosis'] = stats.kurtosis(signal_denoised) \n",
    "    \n",
    "        # Hjorth Parameters\n",
    "        activity, mobility, complexity = hjorth_parameters(signal_denoised)\n",
    "        features['hjorth_activity'] = activity\n",
    "        features['hjorth_mobility'] = mobility\n",
    "        features['hjorth_complexity'] = complexity\n",
    "    \n",
    "    \n",
    "        # Proceed with advanced features only if the signal is long enough\n",
    "        if len(signal_denoised) >= sampling_rate * 2:\n",
    "            # ECG Processing using NeuroKit2\n",
    "            ecg_signals, ecg_info = nk.ecg_process(signal_denoised, sampling_rate=sampling_rate)\n",
    "    \n",
    "            r_peaks = ecg_info['ECG_R_Peaks']\n",
    "    \n",
    "            if len(r_peaks) >= 4:  # Need at least 4 R-peaks for HRV analysis\n",
    "                rri = np.diff(r_peaks) / sampling_rate * 1000 \n",
    "                features['RR_mean'] = np.mean(rri) # is it the same as what is given by the nk.hrv_time ?\n",
    "                \n",
    "                # Time-Domain Features\n",
    "                hrv_time = nk.hrv_time(r_peaks, sampling_rate=sampling_rate, show=False)\n",
    "                features.update(hrv_time.iloc[0].to_dict())\n",
    "    \n",
    "                # Frequency-Domain Features\n",
    "                hrv_freq = nk.hrv_frequency(r_peaks, sampling_rate=sampling_rate, show=False)\n",
    "                features.update(hrv_freq.iloc[0].to_dict())\n",
    "    \n",
    "                # Non-linear Features\n",
    "                # Only if RR intervals are sufficient\n",
    "                if len(rri) >= 10:\n",
    "                    try:\n",
    "                        hrv_nonlinear = nk.hrv_nonlinear(r_peaks, sampling_rate=sampling_rate, show=False)\n",
    "                        features.update(hrv_nonlinear.iloc[0].to_dict())\n",
    "                    except ValueError as e:\n",
    "                        counter = counter +1\n",
    "                        #print(f\"Skipping non-linear features for this signal due to: {e}\")\n",
    "    \n",
    "                # Morphological Features\n",
    "                # PQRST features using Biosppy\n",
    "                ecg_out = ecg.ecg(signal=signal_denoised, sampling_rate=sampling_rate, show=False)\n",
    "                templates = ecg_out['templates']\n",
    "                heart_rate = ecg_out['heart_rate']\n",
    "    \n",
    "                if templates.size > 0 and heart_rate.size > 0:\n",
    "                    # Mean and STD of heart rate\n",
    "                    features['HR_mean'] = np.mean(heart_rate)\n",
    "                    features['HR_std'] = np.std(heart_rate)\n",
    "    \n",
    "                    # Mean and STD of QRS complexes\n",
    "                    features['QRS_mean'] = np.mean(templates)\n",
    "                    features['QRS_std'] = np.std(templates)\n",
    "    \n",
    "                # Calculate PR,QT, and QRS durations\n",
    "                \n",
    "                delineate_signal, delineate_info = nk.ecg_delineate(\n",
    "                    signal_denoised, r_peaks, sampling_rate=sampling_rate, method=\"dwt\", show=False)\n",
    "    \n",
    "                required_keys = ['ECG_P_Onsets', 'ECG_R_Onsets', 'ECG_T_Offsets',\n",
    "                                 'ECG_R_Offsets', 'ECG_T_Onsets', 'ECG_P_Peaks', 'ECG_T_Peaks']\n",
    "                if all(key in delineate_info and len(delineate_info[key]) > 0 for key in required_keys):\n",
    "                    # PR Interval\n",
    "                    pr_interval = ( np.array(delineate_info['ECG_R_Onsets']) - np.array(delineate_info['ECG_P_Onsets'])) / sampling_rate\n",
    "                    features['PR_interval_mean'] = np.mean(pr_interval)\n",
    "                    features['PR_interval_std'] = np.std(pr_interval)\n",
    "    \n",
    "                    # QT Interval\n",
    "                    qt_interval = ( np.array(delineate_info['ECG_T_Offsets']) - np.array(delineate_info['ECG_R_Onsets'])) / sampling_rate\n",
    "                    features['QT_interval_mean'] = np.mean(qt_interval)\n",
    "                    features['QT_interval_std'] = np.std(qt_interval)\n",
    "    \n",
    "                    # QRS Duration\n",
    "                    qrs_duration = (np.array(delineate_info['ECG_R_Offsets']) - np.array(delineate_info['ECG_R_Onsets'])) / sampling_rate\n",
    "                    features['QRS_duration_mean'] = np.mean(qrs_duration)\n",
    "                    features['QRS_duration_std'] = np.std(qrs_duration)\n",
    "    \n",
    "                    # ST Segment\n",
    "                    st_segment = (np.array(delineate_info['ECG_T_Onsets']) - np.array(delineate_info['ECG_R_Offsets'])) / sampling_rate\n",
    "                    features['ST_segment_mean'] = np.mean(st_segment)\n",
    "                    features['ST_segment_std'] = np.std(st_segment)\n",
    "    \n",
    "                    # T-wave amplitude\n",
    "                    ecg_t_peaks = np.array(delineate_info['ECG_T_Peaks'])\n",
    "                    ecg_t_peaks = ecg_t_peaks[~np.isnan(ecg_t_peaks)]\n",
    "                    ecg_t_peaks = ecg_t_peaks.astype(int)\n",
    "                    \n",
    "                    t_waves = signal_denoised[ecg_t_peaks]\n",
    "                    features['T_wave_amplitude_mean'] = np.mean(t_waves)\n",
    "                    features['T_wave_amplitude_std'] = np.std(t_waves)\n",
    "    \n",
    "                    \n",
    "                    # P-wave amplitude\n",
    "                    ecg_p_peaks = np.array(delineate_info['ECG_P_Peaks'])\n",
    "                    ecg_p_peaks = ecg_p_peaks[~np.isnan(ecg_p_peaks)]\n",
    "                    ecg_p_peaks = ecg_p_peaks.astype(int)\n",
    "                    \n",
    "                    p_waves = signal_denoised[ecg_p_peaks]\n",
    "                    features['P_wave_amplitude_mean'] = np.mean(p_waves)\n",
    "                    features['P_wave_amplitude_std'] = np.std(p_waves)\n",
    "    \n",
    "                    # R-wave amplitude\n",
    "                    r_waves = signal_denoised[r_peaks]\n",
    "                    features['R_wave_amplitude_mean'] = np.mean(r_waves)\n",
    "                    features['R_wave_amplitude_std'] = np.std(r_waves)\n",
    "        \n",
    "    except Exception as e:\n",
    "        counter = counter +1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f2405f-e8ad-40dc-abab-7f728e7b82d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not fully prerocessed datapoints: 68\n",
      "columns to drop: 31\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction for Training Data\n",
    "\n",
    "sampling_rate = 300  # Hz\n",
    "train_features = []\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    features = extract_features(row, sampling_rate=sampling_rate)\n",
    "    train_features.append(features)\n",
    "\n",
    "# Convert the list of feature dictionaries to a DataFrame\n",
    "train_feature_df = pd.DataFrame(train_features)\n",
    "proportion_of_missing_values= train_feature_df.isnull().mean()\n",
    "columns_to_drop = proportion_of_missing_values[proportion_of_missing_values>0.2].index\n",
    "print(f'Not fully prerocessed datapoints: {counter}')\n",
    "print(f'columns to drop: {len(columns_to_drop)}')\n",
    "train_feature_df= train_feature_df.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a628cb2d-d6fd-44b6-b83f-490014253916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace nan values\n",
    "train_feature_df = pd.DataFrame(train_feature_df)\n",
    "train_feature_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_feature_df.fillna(train_feature_df.mean(), inplace=True)\n",
    "\n",
    "#Standardize the data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(train_feature_df)\n",
    "train_feature_df= scaler.transform(train_feature_df)\n",
    "\n",
    "#dimensionality reduction\n",
    "train_feature_df= SelectKBest(k=30).fit_transform(train_feature_df, train_labels)\n",
    "\n",
    "train_feature_df = pd.DataFrame(train_feature_df)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "train_feature_df = train_feature_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8915d491-edd9-4909-8230-d34edd308d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_ids = test_data['id']\n",
    "test_data = test_data.drop(columns=['id'])\n",
    "\n",
    "# Feature Extraction for Test Data\n",
    "test_features = []\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    features = extract_features(row, sampling_rate=sampling_rate)\n",
    "    test_features.append(features)\n",
    "\n",
    "# Convert the list of feature dictionaries to a DataFrame\n",
    "test_feature_df = pd.DataFrame(test_features)\n",
    "\n",
    "test_feature_df= test_feature_df.drop(columns=columns_to_drop)\n",
    "test_feature_df = pd.DataFrame(test_feature_df)\n",
    "#deal with inf\n",
    "test_feature_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_feature_df.fillna(test_feature_df.mean(), inplace=True)\n",
    "#Scaling\n",
    "test_feature_df= scaler.transform(test_feature_df)\n",
    "test_feature_df = pd.DataFrame(test_feature_df)\n",
    "\n",
    "\n",
    "# Ensure the test set has the same features as the training set\n",
    "missing_cols = set(train_feature_df.columns) - set(test_feature_df.columns)\n",
    "for c in missing_cols:\n",
    "    test_feature_df[c] = np.nan\n",
    "\n",
    "extra_cols = set(test_feature_df.columns) - set(train_feature_df.columns)\n",
    "test_feature_df = test_feature_df.drop(columns=extra_cols)\n",
    "\n",
    "test_feature_df = test_feature_df[train_feature_df.columns]\n",
    "\n",
    "\n",
    "test_feature_df = pd.DataFrame(test_feature_df)\n",
    "\n",
    "# Reset indices\n",
    "test_ids = test_ids.reset_index(drop=True)\n",
    "test_feature_df = test_feature_df.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea47607-b989-4356-a649-87c1a58c71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Ensure Feature DataFrames Are Not Empty\n",
    "if train_feature_df.empty or test_feature_df.empty:\n",
    "    raise ValueError(\"Problem, check again the extraction.\")\n",
    "\n",
    "if train_feature_df.shape[1] == 0:\n",
    "    raise ValueError(\"No features. check feature extraction.\")\n",
    "\n",
    "\n",
    "test_ids.to_csv('test_ids.csv')\n",
    "test_feature_df.to_csv('test_features.csv')\n",
    "train_labels.to_csv('train_labels.csv')\n",
    "train_feature_df.to_csv('train_features.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
